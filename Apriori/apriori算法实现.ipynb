{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Support  Confidence\n",
      "Items                            Recommend                            \n",
      "2BPM007                          1ADS002          0.047619    0.666667\n",
      "                                 2BJH005          0.047619    0.666667\n",
      "                                 2BPY448          0.047619    0.666667\n",
      "                                 2BJH005,2BPY448  0.047619    0.666667\n",
      "1ADS002                          2BPM007          0.047619    1.000000\n",
      "...                                                    ...         ...\n",
      "2BYP206,2BYW001-,2BYW013,2BYX029 2BYD011          0.047619    1.000000\n",
      "2BYD011,2BYW001-,2BYW013,2BYX029 2BYP206          0.047619    1.000000\n",
      "2BYD011,2BYP206,2BYW013,2BYX029  2BYW001-         0.047619    1.000000\n",
      "2BYD011,2BYP206,2BYW001-,2BYX029 2BYW013          0.047619    1.000000\n",
      "2BYD011,2BYP206,2BYW001-,2BYW013 2BYX029          0.047619    1.000000\n",
      "\n",
      "[467 rows x 2 columns]\n",
      "Recommend\n",
      "2BYD011    0.724888\n",
      "2BYY023    0.502948\n",
      "2BYL019    0.502948\n",
      "2BYD009    0.501895\n",
      "Name: Weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import os,itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Apriori(object):\n",
    "    def __init__(self, itemSets, minSupport=0.5, minConf=0.7, sort = False):\n",
    "        self.itemSets = itemSets\n",
    "        self.minSupport = minSupport\n",
    "        self.minConf = minConf\n",
    "        self.sort = sort\n",
    "        self.__Initialize()\n",
    "\n",
    "    def __Initialize(self):\n",
    "        self.__item()\n",
    "        self.__creat_matrix()\n",
    "        self.update(minSupport=self.minSupport, minConf=self.minConf)\n",
    "\n",
    "    def __item(self):\n",
    "        '''获取项目元素列表'''\n",
    "        self.item = []\n",
    "        for itemSet in self.itemSets:\n",
    "            for item in itemSet:\n",
    "                if item not in self.item:\n",
    "                    self.item.append(item)\n",
    "        self.item.sort()\n",
    "\n",
    "    def __creat_matrix(self):\n",
    "        '''将项集转为pandas.DataFrame数据类型'''\n",
    "        self.data = pd.DataFrame(columns=self.item)\n",
    "        for i in range(len(self.itemSets)):\n",
    "            self.data.loc[i, self.itemSets[i]] = 1\n",
    "\n",
    "    def __candidate_itemsets_l1(self):\n",
    "        '''创建单项频繁项集及L1'''\n",
    "        self.L1 = self.data.loc[:, self.data.sum(axis=0) / len(self.itemSets) >= self.minSupport]\n",
    "        self.L1_support_selects = dict(self.L1.sum(axis=0) / len(self.itemSets))  # 只作为分母，不进行拆分\n",
    "\n",
    "    def __candidate_itemsets_lk(self):\n",
    "        '''根据L1创建多项频繁项集Lk，非频繁项集的任何超集都不是频繁项集'''\n",
    "        last_support_selects = self.L1_support_selects.copy()  # 初始化\n",
    "        while last_support_selects:\n",
    "            new_support_selects = {}\n",
    "            for last_support_select in last_support_selects.keys():\n",
    "                for L1_support_name in set(self.L1.columns) - set(last_support_select.split(',')):\n",
    "                    columns = sorted([L1_support_name] + last_support_select.split(','))  # 新的列名：合并后排序\n",
    "                    count = (self.L1.loc[:, columns].sum(axis=1) == len(columns)).sum()\n",
    "                    if count / len(self.itemSets) >= self.minSupport:\n",
    "                        new_support_selects[','.join(columns)] = count / len(self.itemSets)\n",
    "            self.support_selects.update(new_support_selects)\n",
    "            last_support_selects = new_support_selects.copy()  # 作为新的 Lk，进行下一轮更新\n",
    "\n",
    "    def __support_selects(self):\n",
    "        '''支持度选择'''\n",
    "        self.__candidate_itemsets_l1()\n",
    "        self.__candidate_itemsets_lk()\n",
    "        self.item_Conf = self.L1_support_selects.copy()\n",
    "        self.item_Conf.update(self.support_selects)\n",
    "\n",
    "    def __confidence_selects(self):\n",
    "        '''生成关联规则，其中support_selects已经按照长度大小排列'''\n",
    "        for groups, Supp_groups in self.support_selects.items():\n",
    "            groups_list = groups.split(',')\n",
    "            for recommend_len in range(1, len(groups_list)):\n",
    "                for recommend in itertools.combinations(groups_list, recommend_len):\n",
    "                    items = ','.join(sorted(set(groups_list) - set(recommend)))\n",
    "                    Conf = Supp_groups / self.item_Conf[items]\n",
    "                    if Conf >= self.minConf:\n",
    "                        self.confidence_select.setdefault(items, {})\n",
    "                        self.confidence_select[items].setdefault(','.join(recommend),{'Support': Supp_groups, 'Confidence': Conf})\n",
    "\n",
    "    def show(self,**kwargs):\n",
    "        '''可视化输出'''\n",
    "        if kwargs.get('data'):\n",
    "            select = kwargs['data']\n",
    "        else:\n",
    "            select = self.confidence_select\n",
    "        items = []\n",
    "        value = []\n",
    "        for ks, vs in select.items():\n",
    "            items.extend(list(zip([ks] * vs.__len__(), vs.keys())))\n",
    "            for v in vs.values():\n",
    "                value.append([v['Support'], v['Confidence']])\n",
    "        index = pd.MultiIndex.from_tuples(items, names=['Items', 'Recommend'])\n",
    "        self.rules = pd.DataFrame(value, index=index, columns=['Support', 'Confidence'])\n",
    "        if self.sort or kwargs.get('sort'):\n",
    "            result = self.rules.sort_values(by=['Support', 'Confidence'], ascending=False)\n",
    "        else:\n",
    "            result = self.rules.copy()\n",
    "        return result\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        '''用于更新数据'''\n",
    "        if kwargs.get('minSupport'):\n",
    "            self.minSupport = kwargs['minSupport']\n",
    "            self.support_selects = {}  # 用于储存满足支持度的频繁项集\n",
    "            self.__support_selects()\n",
    "        if kwargs.get('minConf'):\n",
    "            self.minConf = kwargs['minConf']\n",
    "            self.confidence_select = {}  # 用于储存满足自信度的关联规则\n",
    "            self.__confidence_selects()\n",
    "        print(self.show())\n",
    "        if kwargs.get('file_name'):\n",
    "            file_name = kwargs['file_name']\n",
    "            if file_name.endswith(\".xlsx\"):\n",
    "                  self.show().to_excel(f'{file_name}')\n",
    "            else:\n",
    "                  self.show().to_excel(f'{file_name}.xlsx')\n",
    "            \n",
    "        self.apriori_rules = self.rules.copy()\n",
    "\n",
    "    def __get_Recommend_list(self,itemSet):\n",
    "        '''输入数据，获取关联规则列表'''\n",
    "        self.recommend_selects = {}\n",
    "        itemSet = set(itemSet) & set(self.apriori_rules.index.levels[0])\n",
    "        if itemSet:\n",
    "            for start_str in itemSet:\n",
    "                for end_str in self.apriori_rules.loc[start_str].index:\n",
    "                    start_list = start_str.split(',')\n",
    "                    end_list = end_str.split(',')\n",
    "                    self.__creat_Recommend_list(start_list, end_list, itemSet)\n",
    "\n",
    "    def __creat_Recommend_list(self,start_list,end_list,itemSet):\n",
    "        '''迭代创建关联规则列表'''\n",
    "        if set(end_list).issubset(itemSet):\n",
    "            start_str = ','.join(sorted(start_list+end_list))\n",
    "            if start_str in self.apriori_rules.index.levels[0]:\n",
    "                for end_str in self.apriori_rules.loc[start_str].index:\n",
    "                    start_list = start_str.split(',')\n",
    "                    end_list = end_str.split(',')\n",
    "                    self.__creat_Recommend_list(sorted(start_list),end_list,itemSet)\n",
    "        elif not set(end_list) & itemSet:\n",
    "            start_str = ','.join(start_list)\n",
    "            end_str = ','.join(end_list)\n",
    "            self.recommend_selects.setdefault(start_str, {})\n",
    "            self.recommend_selects[start_str].setdefault(end_str, {'Support': self.apriori_rules.loc[(start_str, end_str), 'Support'], 'Confidence': self.apriori_rules.loc[(start_str, end_str), 'Confidence']})\n",
    "\n",
    "    def get_Recommend(self,itemSet,**kwargs):\n",
    "        '''获取加权关联规则'''\n",
    "        self.recommend = {}\n",
    "        self.__get_Recommend_list(itemSet)\n",
    "        self.show(data = self.recommend_selects)\n",
    "        items = self.rules.index.levels[0]\n",
    "        for item_str in items:\n",
    "            for recommends_str in self.rules.loc[item_str].index:\n",
    "                recommends_list = recommends_str.split(',')\n",
    "                for recommend_str in recommends_list:\n",
    "                    self.recommend.setdefault(recommend_str,0)\n",
    "                    self.recommend[recommend_str] += self.rules.loc[(item_str,recommends_str),'Support'] * self.rules.loc[(item_str,recommends_str),'Confidence'] * self.rules.loc[item_str,'Support'].mean()/(self.rules.loc[item_str,'Support'].sum()*len(recommends_list))\n",
    "        result = pd.Series(self.recommend,name='Weight').sort_values(ascending=False)\n",
    "        result.index.name = 'Recommend'\n",
    "        result = result/result.sum()\n",
    "        result = 1/(1+np.exp(-result))\n",
    "        print(result)\n",
    "        if kwargs.get('file_name'):\n",
    "            file_name = kwargs['file_name']\n",
    "            if file_name.endswith(\".xlsx\"):\n",
    "                  excel_writer = pd.ExcelWriter(f'{file_name}')\n",
    "            else:\n",
    "                  excel_writer = pd.ExcelWriter(f'{file_name}.xlsx')\n",
    "            result.to_excel(excel_writer,'推荐项目及权重')\n",
    "            self.rules.to_excel(excel_writer, '关联规则树状表')\n",
    "            self.show().to_excel(excel_writer, '总关联规则树状表')\n",
    "            self.show(sort = True).to_excel(excel_writer, '总关联规则排序表')\n",
    "            excel_writer.save()\n",
    "        return result\n",
    "\n",
    "def str2itemsets(strings, split=','):\n",
    "    '''将字符串列表转化为对应的集合'''\n",
    "    itemsets = []\n",
    "    for string in strings:\n",
    "        itemsets.append(sorted(string.split(split)))\n",
    "    return itemsets\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1.导入数据\n",
    "    data = pd.read_excel('/home/weijunfei/算法/Apriori/apriori算法实现.xlsx' )\n",
    "\n",
    "    # 2.关联规则中不考虑多次购买同一件物品，删除重复数据\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    # 3.初始化列表\n",
    "    itemSets = []\n",
    "\n",
    "    # 3.按销售单分组，只有1件商品的没有意义，需要进行过滤\n",
    "    groups = data.groupby(by='销售单明细')\n",
    "    for group in groups:\n",
    "        if len(group[1]) >= 2:\n",
    "            itemSets.append(group[1]['商品编码'].tolist())\n",
    "\n",
    "    # 4.训练 Apriori\n",
    "    ap = Apriori(itemSets, minSupport=0.03, minConf=0.5)\n",
    "    ap.get_Recommend('2BYP206,2BYW001-,2BYW013,2BYX029'.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class apriori_algorithm:\n",
    " \n",
    "    # 算法初始化\n",
    "    def __init__(self, minSupport, dataSet):\n",
    "        self.minSupport = minSupport  # 最小支持度\n",
    "        self.dataSet = dataSet  # 数据集\n",
    " \n",
    "    # 加载数据集\n",
    "    def loaddata(self):\n",
    "        return edges\n",
    " \n",
    "    # 生成单个物品的项集列表\n",
    "    def generateC1(self, dataSet):\n",
    "        C1 = []  # 用于存放生成的单个物品的项集列表\n",
    "        # 遍历数据集\n",
    "        for data in dataSet:\n",
    "            for item in data:\n",
    "                if [item] not in C1:\n",
    "                    C1.append([item])\n",
    "        C1.sort()\n",
    "        return C1\n",
    " \n",
    "    # 遍历数据集，和Ck对比，计数\n",
    "    def generateLk_by_Ck(self, dataSet, Ck, minSupport, support_data):\n",
    "        \"\"\"\n",
    "           Generate Lk by executing a delete policy from Ck.\n",
    "           Args:\n",
    "               data_set: 数据集\n",
    "               Ck: A set which contains all all frequent candidate k-itemsets.\n",
    "               min_support: The minimum support.\n",
    "               support_data: A dictionary. The key is frequent itemset and the value is support.\n",
    "           Returns:\n",
    "               Lk: A set which contains all all frequent k-itemsets.\n",
    "           \"\"\"\n",
    "        D = map(set, dataSet)\n",
    "        C = map(frozenset, Ck)\n",
    "        C1 = list(C)  # 关于map对象的遍历，在内循环中遍历完最后一个元素后，再次访问时会放回空列表，所以外循环第二次进入的时候是空的，需要将其转为list处理\n",
    "        countData = dict()\n",
    "        for d in D:  # set遍历\n",
    "            for c in C1:\n",
    "                if c.issubset(d):  # 子集判断，并非元素判断\n",
    "                    if c not in countData.keys():  # 将集合作为字典的键使用,c为[]型\n",
    "                        countData[c] = 1\n",
    " \n",
    "                    else:\n",
    "                        countData[c] += 1\n",
    " \n",
    "        numItems = float(len(list(dataSet)))\n",
    "        returnList = []\n",
    "        supportData = dict()\n",
    "        # 遍历前面得到的计数字典\n",
    "        for key in countData:\n",
    "            support = countData[key] / numItems\n",
    "            if support >= minSupport:\n",
    "                returnList.insert(0, key)  # insert() 函数用于将指定对象插入列表的指定位置\n",
    "                support_data[key] = support\n",
    " \n",
    "        return returnList\n",
    " \n",
    "    def generate_L(self, dataSet, k, min_support):\n",
    "        \"\"\"\n",
    "           Generate all frequent itemsets.\n",
    "           Args:\n",
    "               data_set:数据集\n",
    "               k: 频繁项集中含有的最多的元素\n",
    "               min_support: 最小支持度\n",
    "           Returns:\n",
    "               L: 出现的所有频繁项集\n",
    "               support_data: 每个频繁项集对应的支持度\n",
    "           \"\"\"\n",
    "        support_data = {}\n",
    "        C1 = self.generateC1(dataSet)\n",
    "        L1 = self.generateLk_by_Ck(dataSet, C1, min_support, support_data)\n",
    "        Lksub1 = L1.copy()\n",
    " \n",
    "        L = []\n",
    "        L.append(Lksub1)\n",
    " \n",
    "        for i in range(2, k + 1):\n",
    "            Ci = self.generateCK(Lksub1, i)\n",
    "            Li = self.generateLk_by_Ck(dataSet, Ci, min_support, support_data)\n",
    "            Lksub1 = Li.copy()\n",
    "            L.append(Lksub1)\n",
    "        return L, support_data\n",
    " \n",
    "    # generateCK 候选频繁项集产生   参数 Lk频繁项集，k:项集元素个数\n",
    "    def generateCK(self, Lk, k):\n",
    "        Ck = set()\n",
    "        len_Lk = len(list(Lk))\n",
    "        list_Lk = list(Lk)\n",
    "        for i in range(len_Lk):\n",
    "            for j in range(1, len_Lk):\n",
    "                l1 = list(list_Lk[i])\n",
    "                l2 = list(list_Lk[j])\n",
    "                l1.sort()\n",
    "                l2.sort()\n",
    "                if l1[0:k - 2] == l2[0:k - 2]:\n",
    "                    Ck_item = list_Lk[i] | list_Lk[j]\n",
    "                    if self.isCk(Ck_item, list_Lk):\n",
    "                        Ck.add(Ck_item)\n",
    "                    # Ck.add(Ck_item)\n",
    "        return Ck\n",
    " \n",
    "    # 频繁项集判断\n",
    " \n",
    "    def isCk(self, Ck_item, list_Lk):\n",
    "        for item in Ck_item:\n",
    "            sub_Ck = Ck_item - frozenset([item])\n",
    "            if sub_Ck not in list_Lk:\n",
    "                return False\n",
    "        return True\n",
    " \n",
    "    # 生成关联规则\n",
    "    def generate_big_rules(self, L, support_data, min_conf):\n",
    "        \"\"\"\n",
    "        Generate big rules from frequent itemsets.\n",
    "        Args:\n",
    "            L: 所有频繁项集的列表\n",
    "            support_data: 每个频繁项集对应的支持度\n",
    "            min_conf: 最小可信度\n",
    "        \"\"\"\n",
    "        big_rule_list = []\n",
    "        sub_set_list = []\n",
    "        for i in range(0, len(L)):\n",
    "            for freq_set in L[i]:\n",
    "                for sub_set in sub_set_list:\n",
    "                    if sub_set.issubset(freq_set):\n",
    "                        conf = support_data[freq_set] / support_data[freq_set - sub_set]\n",
    "                        big_rule = (freq_set - sub_set, sub_set, conf)\n",
    " \n",
    "                        if conf >= min_conf and big_rule not in big_rule_list:\n",
    "                            print(freq_set - sub_set, \" => \", sub_set, \"conf: \", conf)\n",
    "                            big_rule_list.append(big_rule)\n",
    "                sub_set_list.append(freq_set)\n",
    "        return big_rule_list\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    minS = 0.0001\n",
    "    dataSet = edges\n",
    "    apriori = apriori_algorithm(minSupport=minS, dataSet=dataSet)\n",
    " \n",
    "    L, support_data = apriori.generate_L(dataSet, 4, minS)\n",
    " \n",
    "    print(support_data)\n",
    "    big_rule_list = apriori.generate_big_rules(L, support_data, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
